import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPRegressor
from bayes_opt import BayesianOptimization
from numpy import loadtxt
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

# Load the dataset
dataset = np.loadtxt('Basic McKee data.csv', delimiter=',')

# Split into input (X) and output (y) variables
X = dataset[:,0:5]
y = dataset[:,5]

# Reshape arrays into rows and cols
X = X.reshape(3009, 5)
y = y.reshape(3009, 1)

# Scale the input and output variables
scale_X = MinMaxScaler()
X = scale_X.fit_transform(X)
scale_y = MinMaxScaler()
y = scale_y.fit_transform(y)

# Define Empirical Formulae for Hebb's Rule
def empirical_formulae_hebb(X_train, y_train):
    n = X_train.shape[0]
    d = X_train.shape[1]
    alpha = 2.2
    beta = 0.3
    gamma = 1.0
    k = 1.0
    m = int(np.round(alpha * np.power(n, beta)))
    p = int(np.round(gamma * d * np.power(n, -1/(2*beta+d))))
    if p < k:
        p = k
    if p > n:
        p = n
    return (m, p)

# Define function to train and evaluate MLPRegressor
def train_evaluate_MLPRegressor(m, p, X_train, y_train, X_test, y_test):
    model = MLPRegressor(hidden_layer_sizes=(m, p), activation='tanh', solver='lbfgs')
    model.fit(X_train, y_train.ravel())
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    return mse

# Split dataset into training and testing sets
train_ratio = 0.7
n_train = int(train_ratio * X.shape[0])
X_train = X[:n_train]
y_train = y[:n_train]
X_test = X[n_train:]
y_test = y[n_train:]

# Determine optimal number of neurons using Empirical Formulae with Hebb's rule
m, p = empirical_formulae_hebb(X_train, y_train)
mse = train_evaluate_MLPRegressor(m, p, X_train, y_train, X_test, y_test)

# Print results
print(f"Optimal number of neurons: {m} and {p}")
print(f"Mean squared error: {mse}")
