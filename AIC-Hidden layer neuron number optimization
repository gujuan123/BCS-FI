import numpy as np
import pandas as pd
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import MinMaxScaler
from scipy.stats import norm

# Load the dataset
dataset = pd.read_csv('Basic McKee data.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

# Scale the input and output variables
scale_X = MinMaxScaler()
X = scale_X.fit_transform(X)
scale_y = MinMaxScaler()
y = scale_y.fit_transform(y.reshape(-1, 1)).ravel()

# Create a function to compute the AIC
def compute_aic(n, mse, k):
    rss = n * mse
    aic = n * np.log(rss/n) + 2 * k
    return aic

# Define the range of hidden neurons to search
min_neurons = 1
max_neurons = 150

# Initialize variables to store the best model and its AIC
best_model = None
best_aic = np.inf

# Search for the optimal number of neurons
for n_neurons in range(min_neurons, max_neurons+1):
    # Train the model
    model = MLPRegressor(hidden_layer_sizes=(n_neurons,))
    model.fit(X, y)
    y_pred = model.predict(X)
    mse = np.mean((y - y_pred)**2)
    k = (X.shape[1] + 1) * n_neurons
    aic = compute_aic(X.shape[0], mse, k)
    
    # Check if the model is the best so far
    if aic < best_aic:
        best_model = model
        best_aic = aic
        best_neurons = n_neurons

# Print the results
print("Best number of neurons:", best_neurons)
print("AIC:", best_aic)
