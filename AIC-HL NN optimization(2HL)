from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, make_scorer
import numpy as np
import itertools

# Load the dataset
dataset = np.loadtxt('Basic McKee data.csv', delimiter=',')

# Split into input (X) and output (y) variables
X = dataset[:,0:5]
y = dataset[:,5]

# Reshape arrays into rows and cols
X = X.reshape(3009, 5)
y = y.reshape(3009, 1)

# Scale the input and output variables
scale_X = MinMaxScaler()
X = scale_X.fit_transform(X)
scale_y = MinMaxScaler()
y = scale_y.fit_transform(y)

# Define function to calculate AIC
def aic(n, mse, k):
    return n * np.log(mse) + 2 * k

# Define a list of hidden layer sizes to try
hidden_layer_sizes = [(i, j) for i, j in itertools.product(range(1, 150), range(1, 150))]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize minimum AIC and best hidden layer sizes
min_aic = np.inf
best_hidden_layer_sizes = None

# Loop over all hidden layer sizes
for hidden_layer_size in hidden_layer_sizes:
    # Train MLP regressor with current hidden layer sizes
    model = MLPRegressor(hidden_layer_sizes=hidden_layer_size, max_iter=1000, random_state=42)
    model.fit(X_train, y_train.ravel())

    # Make predictions on testing set
    y_pred = model.predict(X_test)

    # Calculate mean squared error (MSE) and number of parameters (k)
    mse = mean_squared_error(y_test, y_pred)
    k = model.coefs_[0].size + model.coefs_[1].size + model.intercepts_[0].size + model.intercepts_[1].size

    # Calculate AIC and check if it's the minimum so far
    current_aic = aic(len(y_test), mse, k)
    if current_aic < min_aic:
        min_aic = current_aic
        best_hidden_layer_sizes = hidden_layer_size

# Print the best hidden layer sizes and minimum AIC
print("Best hidden layer sizes: ", best_hidden_layer_sizes)
print("Minimum AIC: ", min_aic)
